{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README\n",
    "\n",
    "Ce projet est essentiellement une classe de AutoML qui se concentrent sur la découverte d'une architecture de réseau de neurones. \n",
    "\n",
    "L'objectif principal est ici de rendre le ML aussi simple que possible : Il s'agit d'une seule classe avec une seule fonction \"fit\". Il suffit d'entrer vos données, et cette méthode retournera un modèle de Keras entrainé. \n",
    "\n",
    "Il tente de traiter tous les problèmes qu'un ingénieur en apprentissage machine devra également traiter:\n",
    "\n",
    "- Hyperparameters tunning\n",
    "- Under/Over-Fitting\n",
    "\n",
    "Cela permet également d'augmenter les périodes d'entraînement au fur et à mesure que les générations s'améliorent, de cette façon nous obtenons de meilleurs résultats.\n",
    "\n",
    "## Running this Jupyter\n",
    "\n",
    "Afin de faire fonctionner ce jupyter notebook, vous devez installer [keras](https://keras.io/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "import random\n",
    "from random import choice\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0) # for reproducibility\n",
    "\n",
    "ACTIVATIONS = ['relu', 'sigmoid', 'softmax', 'tanh', 'elu']\n",
    "OPTIMIZERS = ['sgd' , 'rmsprop', 'adam', 'adadelta', 'adagrad', 'adamax']\n",
    "LOSSES = ['binary_crossentropy', 'categorical_crossentropy', 'sparse_categorical_crossentropy', 'mean_squared_error', 'mean_absolute_error']\n",
    "\n",
    "class Individual:\n",
    "\n",
    "    def __init__(self, input_shape, output_shape, config=None):\n",
    "        \"\"\"\n",
    "        An individual is in the form of a 11 values vector\n",
    "        [n0, n1, n2, n3, n4, n5, n6, n7, n8, n9] where\n",
    "        [n0-n4] are the amount of neurons per layer\n",
    "        [n5-n9] are the activation function on each layer\n",
    "        n10 is the activation function on the last layer\n",
    "        \n",
    "        If any neuron in n0-n4 is 0, it's like having less layers\n",
    "        \"\"\"\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        \n",
    "        self.config = self.__generate_random() if config == None else config\n",
    "        self.nn = self.__build_nn()\n",
    "        self._initial_w = self.nn.get_weights()\n",
    "\n",
    "    \n",
    "    def __build_nn(self):\n",
    "        \n",
    "        model = Sequential()\n",
    "        \n",
    "        for idx, neurons in enumerate(self.config[:5]):\n",
    "            if idx == 0:\n",
    "                model.add(Dense(neurons, activation=self.config[5], input_shape=(self.input_shape,)))\n",
    "            \n",
    "            elif neurons != 0 and idx < self.config[7]:\n",
    "                model.add(Dense(neurons, activation=self.config[5]))\n",
    "            \n",
    "        model.add(Dense(self.output_shape, activation=self.config[6]))\n",
    "        \n",
    "        model.compile(optimizer=self.config[8], loss=self.config[9], metrics=['accuracy'])\n",
    "        \n",
    "        return model\n",
    "        \n",
    "        \n",
    "    def __generate_random(self):\n",
    "\n",
    "        return [\n",
    "            random.randint(8, 64),\n",
    "            random.randint(8, 64),\n",
    "            random.randint(8, 64),\n",
    "            random.randint(8, 64),\n",
    "            random.randint(8, 64),\n",
    "            choice(ACTIVATIONS),\n",
    "            choice(ACTIVATIONS),\n",
    "            random.randint(1, 5),\n",
    "            choice(OPTIMIZERS),\n",
    "            choice(LOSSES)\n",
    "        ]\n",
    "    \n",
    "    def fitness(self, X_train, y_train, X_test, y_test, epochs):\n",
    "        self.nn.set_weights(self._initial_w)\n",
    "        \n",
    "        try:\n",
    "            history = self.nn.fit(X_train, \n",
    "                                  y_train, \n",
    "                                  epochs=epochs, \n",
    "                                  batch_size=256, \n",
    "                                  verbose=0,\n",
    "                                  validation_data=(X_test, y_test))\n",
    "\n",
    "            acc = history.history['accuracy'][-1]\n",
    "            val_acc = history.history['val_accuracy'][-1]\n",
    "            \n",
    "        except: # If we fail to train with given loss function, this prob doesn't fit well :)\n",
    "            return 0\n",
    "        \n",
    "        # We want to avoid overffit so we balance train and test accuracy\n",
    "        return 0.7 * val_acc + 0.3 * acc\n",
    "        \n",
    "\n",
    "class AutoML:\n",
    "    def __init__(self, generations=10, size=10, retain=0.5, mutation_rate=0.10):\n",
    "        \n",
    "        # AutML hyperparameters\n",
    "        self.SIZE = size\n",
    "        self.RETAIN = retain\n",
    "        self.GENERATIONS = generations\n",
    "        self.MUTATION_RATE = mutation_rate\n",
    "        self.BETTER_EVAL = int(0.7*self.GENERATIONS)\n",
    "        self.MAX_EPOCHS = 16\n",
    "        \n",
    "        self._retain_int = int(self.SIZE * self.RETAIN)\n",
    "   \n",
    "        self.best_individual = None\n",
    "        self.best_score = -math.inf\n",
    "\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        This method will run a genetic algorithm to find a neural network for your data\n",
    "        returns: The keras model\n",
    "        \"\"\"\n",
    "        \n",
    "        self.input_shape = X.shape[1]\n",
    "        self.output_shape = y.shape[1]\n",
    "        \n",
    "        self.pop = self.__generate_pop(self.SIZE)\n",
    "        \n",
    "        self.fit_history = []\n",
    "        \n",
    "        split_idx = int(len(X)*0.8)\n",
    "        \n",
    "        X_train = X[:split_idx]\n",
    "        y_train = y[:split_idx]\n",
    "        X_val = X[split_idx:]\n",
    "        y_val = y[split_idx:]\n",
    "        \n",
    "        epochs = 4\n",
    "        \n",
    "        for GENERATION in range(self.GENERATIONS):\n",
    "                        \n",
    "            # after a few iterations we have some good nets usually\n",
    "            # then we start increasing the amount of epochs we train, to get more accurate results\n",
    "            if GENERATION > self.BETTER_EVAL:\n",
    "                epochs = min(epochs + epochs // 2, self.MAX_EPOCHS)\n",
    "                \n",
    "            \n",
    "            # calculate score for each one\n",
    "            scored = [ (p, p.fitness(X_train, y_train, X_val, y_val, epochs)) for p in self.pop ]\n",
    "\n",
    "            # sort the population\n",
    "            sorted_pop = sorted(scored, key=lambda p: p[1], reverse=True)\n",
    "            \n",
    "            avg_fitness = sum([p[1] for p in sorted_pop])/float(self.SIZE)\n",
    "            self.fit_history.append(avg_fitness)\n",
    "            print(f'Average fitness {avg_fitness} on iteration {GENERATION}')\n",
    "\n",
    "            best = sorted_pop[0]\n",
    "\n",
    "            if best[1] > self.best_score:\n",
    "                self.best_score = best[1]\n",
    "                self.best_individual = best[0]\n",
    "\n",
    "            sorted_pop = [v[0] for v in sorted_pop[:self._retain_int]]\n",
    "\n",
    "            # keep only uniques\n",
    "            new_sorted_pop = []\n",
    "            for pop in sorted_pop:\n",
    "                if pop not in new_sorted_pop:\n",
    "                    new_sorted_pop.append(pop)\n",
    "            \n",
    "            sorted_pop = new_sorted_pop\n",
    "\n",
    "            # If we had too many copies...\n",
    "            if len(sorted_pop) < 3:\n",
    "                # add some mutations :)\n",
    "                \n",
    "                for _ in range(4-len(sorted_pop)):\n",
    "                    sorted_pop.append(self.mutate(sorted_pop[0]))\n",
    "            \n",
    "            while len(sorted_pop) < self.SIZE:\n",
    "\n",
    "                new = sorted_pop[0]\n",
    "                while new.config in [i.config for i in sorted_pop]:\n",
    "                    \n",
    "                    idx_p1 = random.randint(0, len(sorted_pop)-1)\n",
    "                    idx_p2 = random.randint(0, len(sorted_pop)-1)\n",
    "                    while idx_p2 == idx_p1:\n",
    "                        idx_p2 = random.randint(0, len(sorted_pop)-1)\n",
    "\n",
    "                    new = self.recombine(sorted_pop[idx_p1], sorted_pop[idx_p2])\n",
    "                    \n",
    "                    # If breeded already in, then try some child mutation...\n",
    "                    if new.config in [i.config for i in sorted_pop]:\n",
    "                        new = self.mutate(new)\n",
    "\n",
    "                sorted_pop.append(new)\n",
    "\n",
    "            assert len(sorted_pop) == self.SIZE\n",
    "\n",
    "            self.pop = []\n",
    "            for i in sorted_pop:\n",
    "                if random.random() < self.MUTATION_RATE:\n",
    "                    self.pop.append(self.mutate(i))\n",
    "                else:\n",
    "                    self.pop.append(i)\n",
    "        \n",
    "        return self.best_individual.nn\n",
    "    \n",
    "    \n",
    "    def recombine(self, p1, p2):\n",
    "        p1 = p1.config\n",
    "        p2 = p2.config\n",
    "        \n",
    "        split_idx = random.randint(0, len(p1))\n",
    "        \n",
    "        child = deepcopy(p1[:split_idx])\n",
    "        child += deepcopy(p2[split_idx:])\n",
    "        \n",
    "        return Individual(self.input_shape, self.output_shape, child)\n",
    "        \n",
    "    def mutate(self, p):\n",
    "        p = p.config\n",
    "        \n",
    "        idx = random.randint(0, len(p)-1) # selects one property to change\n",
    "        cur_value = p[idx]\n",
    "        \n",
    "        while p[idx] == cur_value:      # ensure we are changing\n",
    "            \n",
    "            if idx == 0:\n",
    "                p[idx] = random.randint(1, 64)\n",
    "            elif idx > 0 and idx < 5:\n",
    "                p[idx] = random.randint(0, 64)\n",
    "            elif idx == 5 or idx == 6:\n",
    "                p[idx] = choice(ACTIVATIONS)\n",
    "            elif idx == 7:\n",
    "                p[idx] = random.randint(1, 5)\n",
    "            elif idx == 8:\n",
    "                p[idx] = choice(OPTIMIZERS)\n",
    "            elif idx == 9:\n",
    "                p[idx] = choice(LOSSES)\n",
    "                \n",
    "        return Individual(self.input_shape, self.output_shape, p)\n",
    "    \n",
    "    def __generate_pop(self, size):\n",
    "        pop = []\n",
    "        for _ in range(size):\n",
    "            pop.append(self.__generate_individual())\n",
    "            \n",
    "        return pop\n",
    "            \n",
    "\n",
    "    def __generate_individual(self):\n",
    "        return Individual(self.input_shape, self.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist():\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "    # Normalize the images.\n",
    "    X_train = (X_train / 255.) - 0.5\n",
    "    X_test = (X_test / 255.) - 0.5\n",
    "\n",
    "    # Flatten the images.\n",
    "    X_train = X_train.reshape((-1, 784))\n",
    "    X_test = X_test.reshape((-1, 784))\n",
    "\n",
    "    y_train = keras.utils.to_categorical(y_train)\n",
    "    y_test = keras.utils.to_categorical(y_test)\n",
    "    \n",
    "    return (X_train, y_train), (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "(X_train, y_train), (X_test, y_test) = get_mnist()\n",
    "\n",
    "# Created a AutoML class\n",
    "a = AutoML()\n",
    "\n",
    "# Just let the magic happen :)\n",
    "model = a.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = keras.utils.to_categorical(np.argmax(y_pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set 0.9567\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for idx in range(len(y_pred)):\n",
    "    if (y_pred[idx] == y_test[idx]).all():\n",
    "        correct += 1\n",
    "        \n",
    "print(f'Accuracy on test set {correct/float(len(y_pred))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[58, 50, 27, 38, 36, 'relu', 'softmax', 5, 'rmsprop', 'binary_crossentropy']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.best_individual.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First test on a classification problem\n",
    "\n",
    "The code above run AutoML in a classification problem. Note that we don't need to specify it's a classification problem or anything else. We just give the data and the genetic algorithm makes it figure it out which is the kind of problem to be solved.\n",
    "\n",
    "It's interesting to see that (at least with `random.seed(15)` ) we find some expected results as loss function being some kind of cross_entropy, last activation function is a softmax and we do not need too many layers (as this overfits the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
