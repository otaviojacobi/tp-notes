{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README\n",
    "\n",
    "Ce projet est essentiellement une classe de AutoML qui se concentrent sur la découverte d'une architecture de réseau de neurones. \n",
    "\n",
    "L'objectif principal est ici de rendre le ML aussi simple que possible : Il s'agit d'une seule classe avec une seule fonction \"fit\". Il suffit d'entrer vos données, et cette méthode retournera un modèle de Keras entrainé. \n",
    "\n",
    "Il tente de traiter tous les problèmes qu'un ingénieur en apprentissage machine devra également traiter:\n",
    "\n",
    "- Hyperparameters tunning (batch size, amount of layers, neurons per layer, loss function, optimizer, activation function for hidden and output layer)\n",
    "- Under/Over-Fitting\n",
    "\n",
    "Cela permet également d'augmenter les périodes d'entraînement au fur et à mesure que les générations s'améliorent, de cette façon nous obtenons de meilleurs résultats.\n",
    "\n",
    "## Running this Jupyter\n",
    "\n",
    "Afin de faire fonctionner ce jupyter notebook, vous devez installer [keras](https://keras.io/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import boston_housing\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.datasets import boston_housing\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import random\n",
    "from random import choice\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42 # for reproducibility\n",
    "\n",
    "random.seed(SEED) \n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "ACTIVATIONS = ['relu', 'sigmoid', 'softmax', 'tanh', 'elu', 'linear']\n",
    "OPTIMIZERS = ['sgd' , 'rmsprop', 'adam', 'adadelta', 'adagrad', 'adamax']\n",
    "CLASS_LOSSES = ['categorical_crossentropy', 'binary_crossentropy', 'sparse_categorical_crossentropy']\n",
    "REGR_LOSSES = ['mean_squared_error', 'mean_absolute_error']\n",
    "BATCH_SIZES = [8, 16, 32, 64, 128, 256]\n",
    "\n",
    "class Individual:\n",
    "\n",
    "    def __init__(self, input_shape, output_shape, losses, config=None):\n",
    "        \"\"\"\n",
    "        An individual is in the form of a 11 values vector\n",
    "        [n0, n1, n2, n3, n4, n5, n6, n7, n8, n9, n10] where\n",
    "        [n0-n4] are the amount of neurons per layer\n",
    "        [n5-n6] activation functions on hidden and output layer\n",
    "        [n7] amount of layers to be considered\n",
    "        [n8] optimizer to be used\n",
    "        [n9] loss to be used\n",
    "        [n10] batch size\n",
    "        \n",
    "        If any neuron_in_layer in n0-n4 is 0, it's like having less layers\n",
    "        \"\"\"\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.losses = losses\n",
    "        \n",
    "        self.config = self.__generate_random() if config == None else config\n",
    "        self.nn = self.__build_nn()\n",
    "        self._initial_w = self.nn.get_weights()\n",
    "\n",
    "    \n",
    "    def __build_nn(self):\n",
    "        \n",
    "        model = Sequential()\n",
    "        \n",
    "        for idx, neurons in enumerate(self.config[:5]):\n",
    "            if idx == 0:\n",
    "                model.add(Dense(neurons, activation=self.config[5], input_shape=(self.input_shape,)))\n",
    "            \n",
    "            elif neurons != 0 and idx < self.config[7]:\n",
    "                model.add(Dense(neurons, activation=self.config[5]))\n",
    "            \n",
    "        model.add(Dense(self.output_shape, activation=self.config[6]))\n",
    "        \n",
    "        model.compile(optimizer=self.config[8], loss=self.config[9], metrics=['accuracy'])\n",
    "        \n",
    "        return model\n",
    "        \n",
    "        \n",
    "    def __generate_random(self):\n",
    "\n",
    "        return [\n",
    "            random.randint(8, 64),\n",
    "            random.randint(8, 64),\n",
    "            random.randint(8, 64),\n",
    "            random.randint(8, 64),\n",
    "            random.randint(8, 64),\n",
    "            choice(ACTIVATIONS),\n",
    "            choice(ACTIVATIONS),\n",
    "            random.randint(1, 5),\n",
    "            choice(OPTIMIZERS),\n",
    "            choice(self.losses),\n",
    "            choice(BATCH_SIZES)\n",
    "        ]\n",
    "    \n",
    "    def fitness(self, X_train, y_train, X_test, y_test, epochs):\n",
    "        self.nn.set_weights(self._initial_w)\n",
    "        \n",
    "        try:\n",
    "            history = self.nn.fit(X_train, \n",
    "                                  y_train, \n",
    "                                  epochs=epochs, \n",
    "                                  batch_size=self.config[10], \n",
    "                                  verbose=0,\n",
    "                                  validation_data=(X_test, y_test))\n",
    "\n",
    "            acc = history.history['accuracy'][-1]\n",
    "            val_acc = history.history['val_accuracy'][-1]\n",
    "            loss = history.history['loss'][-1]\n",
    "            val_loss = history.history['val_loss'][-1]\n",
    "            \n",
    "        except: # If we fail to train with given loss function, this prob doesn't fit well :)\n",
    "            return 0\n",
    "        \n",
    "        # We want to avoid overffit so we balance train and test accuracy (or loss)\n",
    "        \n",
    "        # If in classification problem ... use accuracy directly...\n",
    "        if self.losses == CLASS_LOSSES:\n",
    "            return 0.7 * val_acc + 0.3 * acc\n",
    "        # Use the loss  (but do np.sqrt on squared error, bcs we want to be fair between them)\n",
    "        elif self.config[9] == 'mean_squared_error':\n",
    "            return -np.sqrt(val_loss)\n",
    "        elif self.config[9] == 'mean_absolute_error':\n",
    "            return -val_loss\n",
    "        \n",
    "        # sanity check\n",
    "        else:\n",
    "            print('Houston, we have a problem.')\n",
    "\n",
    "class AutoML:\n",
    "    def __init__(self, generations=15, size=10, retain=0.5, mutation_rate=0.10):\n",
    "        \n",
    "        # AutML hyperparameters\n",
    "        self.SIZE = size\n",
    "        self.RETAIN = retain\n",
    "        self.GENERATIONS = generations\n",
    "        self.MUTATION_RATE = mutation_rate\n",
    "        self.BETTER_EVAL = int(0.7*self.GENERATIONS)\n",
    "        self.MAX_EPOCHS = 20\n",
    "        \n",
    "        self._retain_int = int(self.SIZE * self.RETAIN)\n",
    "   \n",
    "        self.best_individual = None\n",
    "        self.best_score = -math.inf\n",
    "\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        This method will run a genetic algorithm to find a neural network for your data\n",
    "        returns: The keras model\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.input_shape = X.shape[1]\n",
    "        except IndexError:\n",
    "            self.input_shape = 1\n",
    "        \n",
    "        try:\n",
    "            self.output_shape = y.shape[1]\n",
    "        except:\n",
    "            self.output_shape = 1\n",
    "        \n",
    "        self.fit_history = []\n",
    "        \n",
    "        split_idx = int(len(X)*0.8)\n",
    "        \n",
    "        X_train = X[:split_idx]\n",
    "        y_train = y[:split_idx]\n",
    "        X_val = X[split_idx:]\n",
    "        y_val = y[split_idx:]\n",
    "        \n",
    "        # allowing for loss discovery without knowing if it was classification or regresion was too much trouble\n",
    "        # so we do some old fashioned (simple) analysis\n",
    "        if len(np.unique(y)) == 2:\n",
    "            print(f'Evaluating problem as a Classification problem')\n",
    "            self.losses = CLASS_LOSSES\n",
    "        else:\n",
    "            print(f'Evaluating problem as a Regression problem')\n",
    "            self.losses = REGR_LOSSES\n",
    "        \n",
    "        self.pop = self.__generate_pop(self.SIZE)\n",
    "        \n",
    "        epochs = 4\n",
    "        \n",
    "        for GENERATION in range(self.GENERATIONS):\n",
    "                        \n",
    "            # after a few iterations we have some good nets usually\n",
    "            # then we start increasing the amount of epochs we train, to get more accurate results\n",
    "            if GENERATION > self.BETTER_EVAL:\n",
    "                epochs = min(int(1.5 * epochs), self.MAX_EPOCHS)\n",
    "            \n",
    "            print(f'Evaluating generation {GENERATION} with {epochs} epochs')\n",
    "            \n",
    "            \n",
    "            # calculate score for each one\n",
    "            scored = [ (p, p.fitness(X_train, y_train, X_val, y_val, epochs)) for p in self.pop ]\n",
    "\n",
    "            # sort the population\n",
    "            sorted_pop = sorted(scored, key=lambda p: p[1], reverse=True)\n",
    "            \n",
    "            avg_fitness = sum([p[1] for p in sorted_pop])/float(self.SIZE)\n",
    "            self.fit_history.append(avg_fitness)\n",
    "            print(f'Average fitness {avg_fitness} on iteration {GENERATION}')\n",
    "\n",
    "            best = sorted_pop[0]\n",
    "\n",
    "            if best[1] > self.best_score:\n",
    "                self.best_score = best[1]\n",
    "                self.best_individual = best[0]\n",
    "\n",
    "            sorted_pop = [v[0] for v in sorted_pop[:self._retain_int]]\n",
    "\n",
    "            # keep only uniques\n",
    "            new_sorted_pop = []\n",
    "            for pop in sorted_pop:\n",
    "                if pop not in new_sorted_pop:\n",
    "                    new_sorted_pop.append(pop)\n",
    "            \n",
    "            sorted_pop = new_sorted_pop\n",
    "\n",
    "            # If we had too many copies...\n",
    "            if len(sorted_pop) < 3:\n",
    "                # add some mutations :)\n",
    "                \n",
    "                for _ in range(4-len(sorted_pop)):\n",
    "                    sorted_pop.append(self.mutate(sorted_pop[0]))\n",
    "            \n",
    "            while len(sorted_pop) < self.SIZE:\n",
    "\n",
    "                new = sorted_pop[0]\n",
    "                while new.config in [i.config for i in sorted_pop]:\n",
    "                    \n",
    "                    idx_p1 = random.randint(0, len(sorted_pop)-1)\n",
    "                    idx_p2 = random.randint(0, len(sorted_pop)-1)\n",
    "                    while idx_p2 == idx_p1:\n",
    "                        idx_p2 = random.randint(0, len(sorted_pop)-1)\n",
    "\n",
    "                    new = self.recombine(sorted_pop[idx_p1], sorted_pop[idx_p2])\n",
    "                    \n",
    "                    # If breeded already in, then try some child mutation...\n",
    "                    if new.config in [i.config for i in sorted_pop]:\n",
    "                        new = self.mutate(new)\n",
    "\n",
    "                sorted_pop.append(new)\n",
    "\n",
    "            assert len(sorted_pop) == self.SIZE\n",
    "\n",
    "            self.pop = []\n",
    "            for i in sorted_pop:\n",
    "                if random.random() < self.MUTATION_RATE:\n",
    "                    self.pop.append(self.mutate(i))\n",
    "                else:\n",
    "                    self.pop.append(i)\n",
    "                    \n",
    "            print('New generation is ready')\n",
    "        \n",
    "        return self.best_individual.nn\n",
    "    \n",
    "    \n",
    "    def recombine(self, p1, p2):\n",
    "        p1 = p1.config\n",
    "        p2 = p2.config\n",
    "        \n",
    "        split_idx = random.randint(0, len(p1))\n",
    "        \n",
    "        child = deepcopy(p1[:split_idx])\n",
    "        child += deepcopy(p2[split_idx:])\n",
    "        \n",
    "        return Individual(self.input_shape, self.output_shape, self.losses, child)\n",
    "        \n",
    "    def mutate(self, p):\n",
    "        p = p.config\n",
    "        \n",
    "        idx = random.randint(0, len(p)-1) # selects one property to change\n",
    "        cur_value = p[idx]\n",
    "        \n",
    "        while p[idx] == cur_value:      # ensure we are changing\n",
    "            \n",
    "            if idx == 0:\n",
    "                p[idx] = random.randint(1, 64)\n",
    "            elif idx > 0 and idx < 5:\n",
    "                p[idx] = random.randint(0, 64)\n",
    "            elif idx == 5 or idx == 6:\n",
    "                p[idx] = choice(ACTIVATIONS)\n",
    "            elif idx == 7:\n",
    "                p[idx] = random.randint(1, 5)\n",
    "            elif idx == 8:\n",
    "                p[idx] = choice(OPTIMIZERS)\n",
    "            elif idx == 9:\n",
    "                p[idx] = choice(self.losses)\n",
    "            elif idx == 10:\n",
    "                p[idx] = choice(BATCH_SIZES)\n",
    "                \n",
    "        return Individual(self.input_shape, self.output_shape, self.losses, p)\n",
    "    \n",
    "    def __generate_pop(self, size):\n",
    "        pop = []\n",
    "        for _ in range(size):\n",
    "            pop.append(self.__generate_individual())\n",
    "            \n",
    "        return pop\n",
    "            \n",
    "\n",
    "    def __generate_individual(self):\n",
    "        return Individual(self.input_shape, self.output_shape, self.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist():\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "    # Normalize the images.\n",
    "    X_train = (X_train / 255.) - 0.5\n",
    "    X_test = (X_test / 255.) - 0.5\n",
    "\n",
    "    # Flatten the images.\n",
    "    X_train = X_train.reshape((-1, 784))\n",
    "    X_test = X_test.reshape((-1, 784))\n",
    "\n",
    "    y_train = keras.utils.to_categorical(y_train)\n",
    "    y_test = keras.utils.to_categorical(y_test)\n",
    "    \n",
    "    return (X_train, y_train), (X_test, y_test)\n",
    "\n",
    "def get_boston():\n",
    "    (X_train, y_train), (X_test, y_test) = boston_housing.load_data()\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    \n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    return (X_train, y_train), (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating problem as a Classification problem\n",
      "Evaluating generation 0 with 4 epochs\n",
      "Average fitness 0.1497781243175268 on iteration 0\n",
      "New generation is ready\n",
      "Evaluating generation 1 with 4 epochs\n",
      "Average fitness 0.36346937537193297 on iteration 1\n",
      "New generation is ready\n",
      "Evaluating generation 2 with 4 epochs\n",
      "Average fitness 0.58926603294909 on iteration 2\n",
      "New generation is ready\n",
      "Evaluating generation 3 with 4 epochs\n",
      "Average fitness 0.8697566705942155 on iteration 3\n",
      "New generation is ready\n",
      "Evaluating generation 4 with 4 epochs\n",
      "Average fitness 0.8903027099370954 on iteration 4\n",
      "New generation is ready\n",
      "Evaluating generation 5 with 4 epochs\n",
      "Average fitness 0.8669274979829789 on iteration 5\n",
      "New generation is ready\n",
      "Evaluating generation 6 with 4 epochs\n",
      "Average fitness 0.9234725028276445 on iteration 6\n",
      "New generation is ready\n",
      "Evaluating generation 7 with 4 epochs\n",
      "Average fitness 0.9417281317710875 on iteration 7\n",
      "New generation is ready\n",
      "Evaluating generation 8 with 4 epochs\n",
      "Average fitness 0.9435216635465622 on iteration 8\n",
      "New generation is ready\n",
      "Evaluating generation 9 with 4 epochs\n",
      "Average fitness 0.9513420784473416 on iteration 9\n",
      "New generation is ready\n",
      "Evaluating generation 10 with 4 epochs\n",
      "Average fitness 0.951753749847412 on iteration 10\n",
      "New generation is ready\n",
      "Evaluating generation 11 with 6 epochs\n",
      "Average fitness 0.7862187513709068 on iteration 11\n",
      "New generation is ready\n",
      "Evaluating generation 12 with 9 epochs\n",
      "Average fitness 0.9668524980545044 on iteration 12\n",
      "New generation is ready\n",
      "Evaluating generation 13 with 13 epochs\n",
      "Average fitness 0.8775493806600568 on iteration 13\n",
      "New generation is ready\n",
      "Evaluating generation 14 with 19 epochs\n",
      "Average fitness 0.9735402107238771 on iteration 14\n",
      "New generation is ready\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "(X_train, y_train), (X_test, y_test) = get_mnist()\n",
    "\n",
    "# Created a AutoML class\n",
    "a = AutoML()\n",
    "\n",
    "# Just let the magic happen :) WARNING: This will take maybe a couple of hours to finish \n",
    "model = a.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = keras.utils.to_categorical(np.argmax(y_pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set 0.9737\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for idx in range(len(y_pred)):\n",
    "    if (y_pred[idx] == y_test[idx]).all():\n",
    "        correct += 1\n",
    "        \n",
    "print(f'Accuracy on test set {correct/float(len(y_pred))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[55,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 42,\n",
       " 'tanh',\n",
       " 'sigmoid',\n",
       " 5,\n",
       " 'adamax',\n",
       " 'categorical_crossentropy',\n",
       " 8]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.pop[0].config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First test on a classification problem\n",
    "\n",
    "The code above run AutoML in a classification problem. Note that we don't need to specify it's a classification problem or anything else. We just give the data and the genetic algorithm makes it figure it out which is the kind of problem to be solved.\n",
    "\n",
    "It's interesting to see that (at least with `SEED = 42` ) we find some expected results as loss function being some kind of cross_entropy, last activation function is a softmax or sigmoid and we have around ~97% of accuracy on testing data, which validates that the model is not overfitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating problem as a Regression problem\n",
      "Evaluating generation 0 with 4 epochs\n",
      "Average fitness -24.177855864471606 on iteration 0\n",
      "New generation is ready\n",
      "Evaluating generation 1 with 4 epochs\n",
      "Average fitness -23.324545308134027 on iteration 1\n",
      "New generation is ready\n",
      "Evaluating generation 2 with 4 epochs\n",
      "Average fitness -22.844962501525877 on iteration 2\n",
      "New generation is ready\n",
      "Evaluating generation 3 with 4 epochs\n",
      "Average fitness -22.76642409312587 on iteration 3\n",
      "New generation is ready\n",
      "Evaluating generation 4 with 4 epochs\n",
      "Average fitness -22.578319272874403 on iteration 4\n",
      "New generation is ready\n",
      "Evaluating generation 5 with 4 epochs\n",
      "Average fitness -22.419365882873535 on iteration 5\n",
      "New generation is ready\n",
      "Evaluating generation 6 with 4 epochs\n",
      "Average fitness -21.898413032027236 on iteration 6\n",
      "New generation is ready\n",
      "Evaluating generation 7 with 4 epochs\n",
      "Average fitness -21.533835411071777 on iteration 7\n",
      "New generation is ready\n",
      "Evaluating generation 8 with 4 epochs\n",
      "Average fitness -20.77032337188721 on iteration 8\n",
      "New generation is ready\n",
      "Evaluating generation 9 with 4 epochs\n",
      "Average fitness -20.402644443511964 on iteration 9\n",
      "New generation is ready\n",
      "Evaluating generation 10 with 4 epochs\n",
      "Average fitness -18.75920158635256 on iteration 10\n",
      "New generation is ready\n",
      "Evaluating generation 11 with 6 epochs\n",
      "Average fitness -10.030448508262634 on iteration 11\n",
      "New generation is ready\n",
      "Evaluating generation 12 with 9 epochs\n",
      "Average fitness -6.182662806327812 on iteration 12\n",
      "New generation is ready\n",
      "Evaluating generation 13 with 13 epochs\n",
      "Average fitness -3.2238311767578125 on iteration 13\n",
      "New generation is ready\n",
      "Evaluating generation 14 with 19 epochs\n",
      "Average fitness -4.663072681427002 on iteration 14\n",
      "New generation is ready\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "(X_train, y_train), (X_test, y_test) = get_boston()\n",
    "\n",
    "# Created a AutoML class\n",
    "a = AutoML()\n",
    "\n",
    "# Just let the magic happen :)\n",
    "model = a.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "mae = tf.keras.losses.MeanAbsoluteError()\n",
    "err = mae(y_test, y_pred).numpy()\n",
    "\n",
    "print(f'Error on test set: {err}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second test on a regression problem\n",
    "\n",
    "Regression problem was definitly harder, but still achievable. Even though this is an easy dataset, it's interesting to watch the behaviour of automatic learning from data without anything else, we just put the that in and get a at least functional model which can be further trained for more epochs, for example."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
