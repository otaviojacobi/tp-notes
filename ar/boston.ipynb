{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import boston_housing\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.datasets import boston_housing\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import random\n",
    "from random import choice\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42 # for reproducibility\n",
    "\n",
    "random.seed(SEED) \n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "ACTIVATIONS = ['relu', 'sigmoid', 'softmax', 'tanh', 'elu', 'linear']\n",
    "OPTIMIZERS = ['sgd' , 'rmsprop', 'adam', 'adadelta', 'adagrad', 'adamax']\n",
    "CLASS_LOSSES = ['categorical_crossentropy', 'binary_crossentropy', 'sparse_categorical_crossentropy']\n",
    "REGR_LOSSES = ['mean_squared_error', 'mean_absolute_error']\n",
    "BATCH_SIZES = [8, 16, 32, 64, 128, 256]\n",
    "\n",
    "class Individual:\n",
    "\n",
    "    def __init__(self, input_shape, output_shape, losses, config=None):\n",
    "        \"\"\"\n",
    "        An individual is in the form of a 11 values vector\n",
    "        [n0, n1, n2, n3, n4, n5, n6, n7, n8, n9, n10] where\n",
    "        [n0-n4] are the amount of neurons per layer\n",
    "        [n5-n6] activation functions on hidden and output layer\n",
    "        [n7] amount of layers to be considered\n",
    "        [n8] optimizer to be used\n",
    "        [n9] loss to be used\n",
    "        [n10] batch size\n",
    "        \n",
    "        If any neuron_in_layer in n0-n4 is 0, it's like having less layers\n",
    "        \"\"\"\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.losses = losses\n",
    "        \n",
    "        self.config = self.__generate_random() if config == None else config\n",
    "        self.nn = self.__build_nn()\n",
    "        self._initial_w = self.nn.get_weights()\n",
    "\n",
    "    \n",
    "    def __build_nn(self):\n",
    "        \n",
    "        model = Sequential()\n",
    "        \n",
    "        for idx, neurons in enumerate(self.config[:5]):\n",
    "            if idx == 0:\n",
    "                model.add(Dense(neurons, activation=self.config[5], input_shape=(self.input_shape,)))\n",
    "            \n",
    "            elif neurons != 0 and idx < self.config[7]:\n",
    "                model.add(Dense(neurons, activation=self.config[5]))\n",
    "            \n",
    "        model.add(Dense(self.output_shape, activation=self.config[6]))\n",
    "        \n",
    "        model.compile(optimizer=self.config[8], loss=self.config[9], metrics=['accuracy'])\n",
    "        \n",
    "        return model\n",
    "        \n",
    "        \n",
    "    def __generate_random(self):\n",
    "\n",
    "        return [\n",
    "            random.randint(8, 64),\n",
    "            random.randint(8, 64),\n",
    "            random.randint(8, 64),\n",
    "            random.randint(8, 64),\n",
    "            random.randint(8, 64),\n",
    "            choice(ACTIVATIONS),\n",
    "            choice(ACTIVATIONS),\n",
    "            random.randint(1, 5),\n",
    "            choice(OPTIMIZERS),\n",
    "            choice(self.losses),\n",
    "            choice(BATCH_SIZES)\n",
    "        ]\n",
    "    \n",
    "    def fitness(self, X_train, y_train, X_test, y_test, epochs):\n",
    "        self.nn.set_weights(self._initial_w)\n",
    "        \n",
    "        try:\n",
    "            history = self.nn.fit(X_train, \n",
    "                                  y_train, \n",
    "                                  epochs=epochs, \n",
    "                                  batch_size=self.config[10], \n",
    "                                  verbose=0,\n",
    "                                  validation_data=(X_test, y_test))\n",
    "\n",
    "            acc = history.history['accuracy'][-1]\n",
    "            val_acc = history.history['val_accuracy'][-1]\n",
    "            loss = history.history['loss'][-1]\n",
    "            val_loss = history.history['val_loss'][-1]\n",
    "            \n",
    "        except: # If we fail to train with given loss function, this prob doesn't fit well :)\n",
    "            return 0\n",
    "        \n",
    "        # We want to avoid overffit so we balance train and test accuracy (or loss)\n",
    "        \n",
    "        # If in classification problem ... use accuracy directly...\n",
    "        if self.losses == CLASS_LOSSES:\n",
    "            return 0.7 * val_acc + 0.3 * acc\n",
    "        # Use the loss  (but do np.sqrt on squared error, bcs we want to be fair between them)\n",
    "        elif self.config[9] == 'mean_squared_error':\n",
    "            return -np.sqrt(val_loss)\n",
    "        elif self.config[9] == 'mean_absolute_error':\n",
    "            return -val_loss\n",
    "        \n",
    "        # sanity check\n",
    "        else:\n",
    "            print('Houston, we have a problem.')\n",
    "\n",
    "class AutoML:\n",
    "    def __init__(self, generations=15, size=10, retain=0.5, mutation_rate=0.10):\n",
    "        \n",
    "        # AutML hyperparameters\n",
    "        self.SIZE = size\n",
    "        self.RETAIN = retain\n",
    "        self.GENERATIONS = generations\n",
    "        self.MUTATION_RATE = mutation_rate\n",
    "        self.BETTER_EVAL = int(0.7*self.GENERATIONS)\n",
    "        self.MAX_EPOCHS = 20\n",
    "        \n",
    "        self._retain_int = int(self.SIZE * self.RETAIN)\n",
    "   \n",
    "        self.best_individual = None\n",
    "        self.best_score = -math.inf\n",
    "\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        This method will run a genetic algorithm to find a neural network for your data\n",
    "        returns: The keras model\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.input_shape = X.shape[1]\n",
    "        except IndexError:\n",
    "            self.input_shape = 1\n",
    "        \n",
    "        try:\n",
    "            self.output_shape = y.shape[1]\n",
    "        except:\n",
    "            self.output_shape = 1\n",
    "        \n",
    "        self.fit_history = []\n",
    "        \n",
    "        split_idx = int(len(X)*0.8)\n",
    "        \n",
    "        X_train = X[:split_idx]\n",
    "        y_train = y[:split_idx]\n",
    "        X_val = X[split_idx:]\n",
    "        y_val = y[split_idx:]\n",
    "        \n",
    "        # allowing for loss discovery without knowing if it was classification or regresion was too much trouble\n",
    "        # so we do some old fashioned (simple) analysis\n",
    "        if len(np.unique(y)) == 2:\n",
    "            print(f'Evaluating problem as a Classification problem')\n",
    "            self.losses = CLASS_LOSSES\n",
    "        else:\n",
    "            print(f'Evaluating problem as a Regression problem')\n",
    "            self.losses = REGR_LOSSES\n",
    "        \n",
    "        self.pop = self.__generate_pop(self.SIZE)\n",
    "        \n",
    "        epochs = 4\n",
    "        \n",
    "        for GENERATION in range(self.GENERATIONS):\n",
    "                        \n",
    "            # after a few iterations we have some good nets usually\n",
    "            # then we start increasing the amount of epochs we train, to get more accurate results\n",
    "            if GENERATION > self.BETTER_EVAL:\n",
    "                epochs = min(int(1.5 * epochs), self.MAX_EPOCHS)\n",
    "            \n",
    "            print(f'Evaluating generation {GENERATION} with {epochs} epochs')\n",
    "            \n",
    "            \n",
    "            # calculate score for each one\n",
    "            scored = [ (p, p.fitness(X_train, y_train, X_val, y_val, epochs)) for p in self.pop ]\n",
    "\n",
    "            # sort the population\n",
    "            sorted_pop = sorted(scored, key=lambda p: p[1], reverse=True)\n",
    "            \n",
    "            avg_fitness = sum([p[1] for p in sorted_pop])/float(self.SIZE)\n",
    "            self.fit_history.append(avg_fitness)\n",
    "            print(f'Average fitness {avg_fitness} on iteration {GENERATION}')\n",
    "\n",
    "            best = sorted_pop[0]\n",
    "\n",
    "            if best[1] > self.best_score:\n",
    "                self.best_score = best[1]\n",
    "                self.best_individual = best[0]\n",
    "\n",
    "            sorted_pop = [v[0] for v in sorted_pop[:self._retain_int]]\n",
    "\n",
    "            # keep only uniques\n",
    "            new_sorted_pop = []\n",
    "            for pop in sorted_pop:\n",
    "                if pop not in new_sorted_pop:\n",
    "                    new_sorted_pop.append(pop)\n",
    "            \n",
    "            sorted_pop = new_sorted_pop\n",
    "\n",
    "            # If we had too many copies...\n",
    "            if len(sorted_pop) < 3:\n",
    "                # add some mutations :)\n",
    "                \n",
    "                for _ in range(4-len(sorted_pop)):\n",
    "                    sorted_pop.append(self.mutate(sorted_pop[0]))\n",
    "            \n",
    "            while len(sorted_pop) < self.SIZE:\n",
    "\n",
    "                new = sorted_pop[0]\n",
    "                while new.config in [i.config for i in sorted_pop]:\n",
    "                    \n",
    "                    idx_p1 = random.randint(0, len(sorted_pop)-1)\n",
    "                    idx_p2 = random.randint(0, len(sorted_pop)-1)\n",
    "                    while idx_p2 == idx_p1:\n",
    "                        idx_p2 = random.randint(0, len(sorted_pop)-1)\n",
    "\n",
    "                    new = self.recombine(sorted_pop[idx_p1], sorted_pop[idx_p2])\n",
    "                    \n",
    "                    # If breeded already in, then try some child mutation...\n",
    "                    if new.config in [i.config for i in sorted_pop]:\n",
    "                        new = self.mutate(new)\n",
    "\n",
    "                sorted_pop.append(new)\n",
    "\n",
    "            assert len(sorted_pop) == self.SIZE\n",
    "\n",
    "            self.pop = []\n",
    "            for i in sorted_pop:\n",
    "                if random.random() < self.MUTATION_RATE:\n",
    "                    self.pop.append(self.mutate(i))\n",
    "                else:\n",
    "                    self.pop.append(i)\n",
    "                    \n",
    "            print('New generation is ready')\n",
    "        \n",
    "        return self.best_individual.nn\n",
    "    \n",
    "    \n",
    "    def recombine(self, p1, p2):\n",
    "        p1 = p1.config\n",
    "        p2 = p2.config\n",
    "        \n",
    "        split_idx = random.randint(0, len(p1))\n",
    "        \n",
    "        child = deepcopy(p1[:split_idx])\n",
    "        child += deepcopy(p2[split_idx:])\n",
    "        \n",
    "        return Individual(self.input_shape, self.output_shape, self.losses, child)\n",
    "        \n",
    "    def mutate(self, p):\n",
    "        p = p.config\n",
    "        \n",
    "        idx = random.randint(0, len(p)-1) # selects one property to change\n",
    "        cur_value = p[idx]\n",
    "        \n",
    "        while p[idx] == cur_value:      # ensure we are changing\n",
    "            \n",
    "            if idx == 0:\n",
    "                p[idx] = random.randint(1, 64)\n",
    "            elif idx > 0 and idx < 5:\n",
    "                p[idx] = random.randint(0, 64)\n",
    "            elif idx == 5 or idx == 6:\n",
    "                p[idx] = choice(ACTIVATIONS)\n",
    "            elif idx == 7:\n",
    "                p[idx] = random.randint(1, 5)\n",
    "            elif idx == 8:\n",
    "                p[idx] = choice(OPTIMIZERS)\n",
    "            elif idx == 9:\n",
    "                p[idx] = choice(self.losses)\n",
    "            elif idx == 10:\n",
    "                p[idx] = choice(BATCH_SIZES)\n",
    "                \n",
    "        return Individual(self.input_shape, self.output_shape, self.losses, p)\n",
    "    \n",
    "    def __generate_pop(self, size):\n",
    "        pop = []\n",
    "        for _ in range(size):\n",
    "            pop.append(self.__generate_individual())\n",
    "            \n",
    "        return pop\n",
    "            \n",
    "\n",
    "    def __generate_individual(self):\n",
    "        return Individual(self.input_shape, self.output_shape, self.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boston():\n",
    "    (X_train, y_train), (X_test, y_test) = boston_housing.load_data()\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    \n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    return (X_train, y_train), (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating problem as a Regression problem\n",
      "Evaluating generation 0 with 4 epochs\n",
      "Average fitness -21.95597129429627 on iteration 0\n",
      "New generation is ready\n",
      "Evaluating generation 1 with 4 epochs\n",
      "Average fitness -20.386927632149686 on iteration 1\n",
      "New generation is ready\n",
      "Evaluating generation 2 with 4 epochs\n",
      "Average fitness -19.431851387023926 on iteration 2\n",
      "New generation is ready\n",
      "Evaluating generation 3 with 4 epochs\n",
      "Average fitness -19.22057695388794 on iteration 3\n",
      "New generation is ready\n",
      "Evaluating generation 4 with 4 epochs\n",
      "Average fitness -17.27517948150635 on iteration 4\n",
      "New generation is ready\n",
      "Evaluating generation 5 with 4 epochs\n",
      "Average fitness -13.240061950683593 on iteration 5\n",
      "New generation is ready\n",
      "Evaluating generation 6 with 4 epochs\n",
      "Average fitness -15.151275300979615 on iteration 6\n",
      "New generation is ready\n",
      "Evaluating generation 7 with 4 epochs\n",
      "Average fitness -15.394983853093487 on iteration 7\n",
      "New generation is ready\n",
      "Evaluating generation 8 with 4 epochs\n",
      "Average fitness -13.171052169799804 on iteration 8\n",
      "New generation is ready\n",
      "Evaluating generation 9 with 4 epochs\n",
      "Average fitness -9.945668649673461 on iteration 9\n",
      "New generation is ready\n",
      "Evaluating generation 10 with 4 epochs\n",
      "Average fitness -5.811750173568726 on iteration 10\n",
      "New generation is ready\n",
      "Evaluating generation 11 with 6 epochs\n",
      "Average fitness -5.579601573944092 on iteration 11\n",
      "New generation is ready\n",
      "Evaluating generation 12 with 9 epochs\n",
      "Average fitness -4.467658810992174 on iteration 12\n",
      "New generation is ready\n",
      "Evaluating generation 13 with 13 epochs\n",
      "Average fitness -6.650703048706054 on iteration 13\n",
      "New generation is ready\n",
      "Evaluating generation 14 with 19 epochs\n",
      "Average fitness -2.831634855270386 on iteration 14\n",
      "New generation is ready\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "(X_train, y_train), (X_test, y_test) = get_boston()\n",
    "\n",
    "# Created a AutoML class\n",
    "a = AutoML()\n",
    "\n",
    "# Just let the magic happen :)\n",
    "model = a.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.729578"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae = tf.keras.losses.MeanAbsoluteError()\n",
    "mae(y_test, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
